{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRDIEdK-_SSo"
      },
      "source": [
        "# DHT22 Prediction Model\n",
        "\n",
        "아래 중간 중간 `#빈칸`을 채우며 적절히 설계해보세요. 끝까지 실행하면 `temp_predict.onnx` 및 `humi_predict.onnx` 파일이 생성됩니다."
      ],
      "id": "QRDIEdK-_SSo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gineZQeU_SSs"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.onnx\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "id": "gineZQeU_SSs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHfk7n1R_SSt"
      },
      "source": [
        "### Class Definitions"
      ],
      "id": "OHfk7n1R_SSt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygr9B-rM_SSt"
      },
      "outputs": [],
      "source": [
        "class DHTDataset(Dataset):\n",
        "    def __init__(self, key):\n",
        "        self.data = []\n",
        "        self.target = []\n",
        "        self.key = key\n",
        "\n",
        "        db = open(\"db.json\", \"r\")\n",
        "        humi_temp = json.load(db)\n",
        "\n",
        "        for i in range(len(humi_temp.get(self.key))-62):\n",
        "            tmp_data = []\n",
        "            tmp_data.append([float(humi_temp.get(self.key)[i][1])])\n",
        "            tmp_data.append([float(humi_temp.get(self.key)[i+1][1])])\n",
        "            tmp_data.append([float(humi_temp.get(self.key)[i+2][1])])\n",
        "            # 연속으로 3번 측정한 값\n",
        "            self.data.append(torch.as_tensor(tmp_data))\n",
        "\n",
        "            self.target.append( #1분뒤 값\n",
        "                torch.as_tensor([float(humi_temp.get(self.key)[i+62][1])]))\n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data, target = self.data[index], self.target[index]\n",
        "        return data, target"
      ],
      "id": "ygr9B-rM_SSt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6655fbbd",
      "metadata": {
        "id": "6655fbbd"
      },
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(3,1)\n",
        "        )\n",
        "        #3가지을 넣어 하나의 값을 받아오는\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self.linear(x.squeeze())\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3bba4f6",
      "metadata": {
        "id": "b3bba4f6"
      },
      "source": [
        "### Function Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3460f0c0",
      "metadata": {
        "id": "3460f0c0"
      },
      "source": [
        "##### A function to split dataset into several data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a92d7b1d",
      "metadata": {
        "id": "a92d7b1d"
      },
      "outputs": [],
      "source": [
        "def splitDataset(dataset):\n",
        "    test_size = int(len(dataset)/6)\n",
        "    val_size = int(len(dataset)/6)\n",
        "    train_size = len(dataset) - test_size - val_size\n",
        "#머신러닝셋 나누기\n",
        "    trainset, valset, testset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size])\n",
        "\n",
        "    trainloader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
        "    validloader = DataLoader(valset, batch_size=16, shuffle=True)\n",
        "    testloader = DataLoader(testset, batch_size=16, shuffle=True)\n",
        "\n",
        "    return trainloader, validloader, testloader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610be662",
      "metadata": {
        "id": "610be662"
      },
      "source": [
        "##### A function to train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "828d4386",
      "metadata": {
        "id": "828d4386"
      },
      "outputs": [],
      "source": [
        "def train_model(model, patience, num_epochs, train_loader, valid_loader):\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    mean_train_losses = []\n",
        "    mean_valid_losses = []\n",
        "    p = 0\n",
        "    min_valid_loss = float(\"inf\")\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "        model.train()\n",
        "        for train_data, train_target in train_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                device = torch.device(\"cuda\")\n",
        "                train_data = train_data.to(device, dtype=torch.float)\n",
        "                train_target = train_target.to(device, dtype=torch.float)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(train_data)\n",
        "            loss = criterion(output, train_target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for valid_data, valid_target in valid_loader:\n",
        "            if torch.cuda.is_available():\n",
        "                device = torch.device(\"cuda\")\n",
        "                valid_data = valid_data.to(device, dtype=torch.float)\n",
        "                valid_target = valid_target.to(device, dtype=torch.float)\n",
        "            output = model(valid_data)\n",
        "            loss = criterion(output, valid_target)\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "        valid_loss = np.mean(valid_losses)\n",
        "        mean_train_losses.append(train_loss)\n",
        "        mean_valid_losses.append(valid_loss)\n",
        "\n",
        "        if min_valid_loss > valid_loss:\n",
        "            min_valid_loss = valid_loss\n",
        "            print(f\"min_valid_loss: {min_valid_loss}\")\n",
        "\n",
        "        epoch_len = len(str(num_epochs))\n",
        "        print_msg = (f\"[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] \" +\n",
        "                     f\"train_loss: {train_loss:.5f} \" +\n",
        "                     f\"valid_loss: {valid_loss:.5f} \")\n",
        "        print(print_msg)\n",
        "\n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "\n",
        "        if min_valid_loss < valid_loss and epoch > 1:\n",
        "            p = p + 1\n",
        "            print(f'patience: {p}')\n",
        "        else:\n",
        "            p = 0\n",
        "            torch.save(model.state_dict(), \"bestmodel.pt\")\n",
        "            print(\"Saving Model...\")\n",
        "\n",
        "        if patience == p:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"bestmodel.pt\"))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83317dae",
      "metadata": {
        "id": "83317dae"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ0Ao0nk_SS2"
      },
      "source": [
        "##### Load datasets and get dataloaders"
      ],
      "id": "WQ0Ao0nk_SS2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go7_9Vzj_SS3"
      },
      "outputs": [],
      "source": [
        "tempDataset = DHTDataset(\"temperature\")\n",
        "humiDataset = DHTDataset(\"humidity\")\n",
        "\n",
        "temp_trainloader, temp_validloader, temp_testloader = splitDataset(tempDataset)\n",
        "humi_trainloader, humi_validloader, humi_testloader = splitDataset(humiDataset)"
      ],
      "id": "Go7_9Vzj_SS3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1toEaEOX_SS3"
      },
      "source": [
        "##### Make models"
      ],
      "id": "1toEaEOX_SS3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H0yN9Bx_SS3"
      },
      "outputs": [],
      "source": [
        "tempModel = SimpleModel()\n",
        "humiModel = SimpleModel()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    tempModel = tempModel.cuda()\n",
        "    humiModel = humiModel.cuda()"
      ],
      "id": "-H0yN9Bx_SS3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x44zKPse_SS4"
      },
      "source": [
        "##### Do training with earlystopping"
      ],
      "id": "x44zKPse_SS4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjUnocuY_SS4",
        "outputId": "fe280028-d631-42dd-8959-6808f064b36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min_valid_loss: 428.7687123616536\n",
            "[   1/1000] train_loss: 505.73347 valid_loss: 428.76871 \n",
            "Saving Model...\n",
            "min_valid_loss: 308.77620188395184\n",
            "[   2/1000] train_loss: 369.53416 valid_loss: 308.77620 \n",
            "Saving Model...\n",
            "min_valid_loss: 216.8365567525228\n",
            "[   3/1000] train_loss: 263.24193 valid_loss: 216.83656 \n",
            "Saving Model...\n",
            "min_valid_loss: 147.85588200887045\n",
            "[   4/1000] train_loss: 182.42872 valid_loss: 147.85588 \n",
            "Saving Model...\n",
            "min_valid_loss: 97.7727165222168\n",
            "[   5/1000] train_loss: 122.78373 valid_loss: 97.77272 \n",
            "Saving Model...\n",
            "min_valid_loss: 62.68458271026611\n",
            "[   6/1000] train_loss: 80.07968 valid_loss: 62.68458 \n",
            "Saving Model...\n",
            "min_valid_loss: 38.78587373097738\n",
            "[   7/1000] train_loss: 50.54128 valid_loss: 38.78587 \n",
            "Saving Model...\n",
            "min_valid_loss: 23.167890866597492\n",
            "[   8/1000] train_loss: 30.79594 valid_loss: 23.16789 \n",
            "Saving Model...\n",
            "min_valid_loss: 13.297178427378336\n",
            "[   9/1000] train_loss: 18.09523 valid_loss: 13.29718 \n",
            "Saving Model...\n",
            "min_valid_loss: 7.371254920959473\n",
            "[  10/1000] train_loss: 10.24764 valid_loss: 7.37125 \n",
            "Saving Model...\n",
            "min_valid_loss: 3.93124520778656\n",
            "[  11/1000] train_loss: 5.59051 valid_loss: 3.93125 \n",
            "Saving Model...\n",
            "min_valid_loss: 2.0082910557587943\n",
            "[  12/1000] train_loss: 2.93482 valid_loss: 2.00829 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.9898176342248917\n",
            "[  13/1000] train_loss: 1.48717 valid_loss: 0.98982 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.4707839364806811\n",
            "[  14/1000] train_loss: 0.72727 valid_loss: 0.47078 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.21966691563526788\n",
            "[  15/1000] train_loss: 0.34506 valid_loss: 0.21967 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.10287795712550481\n",
            "[  16/1000] train_loss: 0.16162 valid_loss: 0.10288 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.05037853308022022\n",
            "[  17/1000] train_loss: 0.07716 valid_loss: 0.05038 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.028815098727742832\n",
            "[  18/1000] train_loss: 0.03999 valid_loss: 0.02882 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.019882889309277136\n",
            "[  19/1000] train_loss: 0.02420 valid_loss: 0.01988 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.01689300554183622\n",
            "[  20/1000] train_loss: 0.01771 valid_loss: 0.01689 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.01617788236277799\n",
            "[  21/1000] train_loss: 0.01530 valid_loss: 0.01618 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.015387131987760464\n",
            "[  22/1000] train_loss: 0.01435 valid_loss: 0.01539 \n",
            "Saving Model...\n",
            "[  23/1000] train_loss: 0.01414 valid_loss: 0.01586 \n",
            "patience: 1\n",
            "min_valid_loss: 0.015341091202571988\n",
            "[  24/1000] train_loss: 0.01397 valid_loss: 0.01534 \n",
            "Saving Model...\n",
            "[  25/1000] train_loss: 0.01391 valid_loss: 0.01550 \n",
            "patience: 1\n",
            "[  26/1000] train_loss: 0.01386 valid_loss: 0.01543 \n",
            "patience: 2\n",
            "[  27/1000] train_loss: 0.01386 valid_loss: 0.01573 \n",
            "patience: 3\n",
            "[  28/1000] train_loss: 0.01389 valid_loss: 0.01589 \n",
            "patience: 4\n",
            "[  29/1000] train_loss: 0.01387 valid_loss: 0.01597 \n",
            "patience: 5\n",
            "[  30/1000] train_loss: 0.01393 valid_loss: 0.01564 \n",
            "patience: 6\n",
            "[  31/1000] train_loss: 0.01385 valid_loss: 0.01557 \n",
            "patience: 7\n",
            "[  32/1000] train_loss: 0.01396 valid_loss: 0.01548 \n",
            "patience: 8\n",
            "[  33/1000] train_loss: 0.01389 valid_loss: 0.01548 \n",
            "patience: 9\n",
            "min_valid_loss: 0.01533772295806557\n",
            "[  34/1000] train_loss: 0.01388 valid_loss: 0.01534 \n",
            "Saving Model...\n",
            "[  35/1000] train_loss: 0.01393 valid_loss: 0.01584 \n",
            "patience: 1\n",
            "[  36/1000] train_loss: 0.01398 valid_loss: 0.01534 \n",
            "patience: 2\n",
            "[  37/1000] train_loss: 0.01389 valid_loss: 0.01576 \n",
            "patience: 3\n",
            "[  38/1000] train_loss: 0.01392 valid_loss: 0.01593 \n",
            "patience: 4\n",
            "[  39/1000] train_loss: 0.01388 valid_loss: 0.01598 \n",
            "patience: 5\n",
            "[  40/1000] train_loss: 0.01391 valid_loss: 0.01574 \n",
            "patience: 6\n",
            "[  41/1000] train_loss: 0.01384 valid_loss: 0.01574 \n",
            "patience: 7\n",
            "[  42/1000] train_loss: 0.01386 valid_loss: 0.01559 \n",
            "patience: 8\n",
            "[  43/1000] train_loss: 0.01386 valid_loss: 0.01573 \n",
            "patience: 9\n",
            "[  44/1000] train_loss: 0.01391 valid_loss: 0.01573 \n",
            "patience: 10\n",
            "[  45/1000] train_loss: 0.01388 valid_loss: 0.01547 \n",
            "patience: 11\n",
            "[  46/1000] train_loss: 0.01395 valid_loss: 0.01553 \n",
            "patience: 12\n",
            "[  47/1000] train_loss: 0.01386 valid_loss: 0.01544 \n",
            "patience: 13\n",
            "[  48/1000] train_loss: 0.01390 valid_loss: 0.01542 \n",
            "patience: 14\n",
            "[  49/1000] train_loss: 0.01389 valid_loss: 0.01618 \n",
            "patience: 15\n",
            "[  50/1000] train_loss: 0.01396 valid_loss: 0.01596 \n",
            "patience: 16\n",
            "[  51/1000] train_loss: 0.01385 valid_loss: 0.01586 \n",
            "patience: 17\n",
            "[  52/1000] train_loss: 0.01386 valid_loss: 0.01566 \n",
            "patience: 18\n",
            "[  53/1000] train_loss: 0.01392 valid_loss: 0.01579 \n",
            "patience: 19\n",
            "[  54/1000] train_loss: 0.01385 valid_loss: 0.01561 \n",
            "patience: 20\n",
            "[  55/1000] train_loss: 0.01411 valid_loss: 0.01588 \n",
            "patience: 21\n",
            "[  56/1000] train_loss: 0.01396 valid_loss: 0.01555 \n",
            "patience: 22\n",
            "[  57/1000] train_loss: 0.01402 valid_loss: 0.01600 \n",
            "patience: 23\n",
            "[  58/1000] train_loss: 0.01392 valid_loss: 0.01593 \n",
            "patience: 24\n",
            "[  59/1000] train_loss: 0.01405 valid_loss: 0.01565 \n",
            "patience: 25\n",
            "[  60/1000] train_loss: 0.01409 valid_loss: 0.01583 \n",
            "patience: 26\n",
            "[  61/1000] train_loss: 0.01397 valid_loss: 0.01571 \n",
            "patience: 27\n",
            "[  62/1000] train_loss: 0.01391 valid_loss: 0.01593 \n",
            "patience: 28\n",
            "[  63/1000] train_loss: 0.01386 valid_loss: 0.01560 \n",
            "patience: 29\n",
            "[  64/1000] train_loss: 0.01396 valid_loss: 0.01568 \n",
            "patience: 30\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "temp_predict_model = train_model(tempModel, patience=30, num_epochs=1000,\n",
        "                                 train_loader=temp_trainloader,\n",
        "                                 valid_loader=temp_validloader)\n",
        "torch.save(temp_predict_model.state_dict(), \"temp_predict.pt\")"
      ],
      "id": "KjUnocuY_SS4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDeF6FwT_SS5",
        "outputId": "fb6baaf2-7af8-4b84-fae8-a5ce570e0095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min_valid_loss: 169.66026306152344\n",
            "[   1/1000] train_loss: 246.14263 valid_loss: 169.66026 \n",
            "Saving Model...\n",
            "min_valid_loss: 75.33166631062825\n",
            "[   2/1000] train_loss: 118.55401 valid_loss: 75.33167 \n",
            "Saving Model...\n",
            "min_valid_loss: 29.043671131134033\n",
            "[   3/1000] train_loss: 49.79583 valid_loss: 29.04367 \n",
            "Saving Model...\n",
            "min_valid_loss: 9.564021110534668\n",
            "[   4/1000] train_loss: 18.04329 valid_loss: 9.56402 \n",
            "Saving Model...\n",
            "min_valid_loss: 2.719623565673828\n",
            "[   5/1000] train_loss: 5.61129 valid_loss: 2.71962 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.767499253153801\n",
            "[   6/1000] train_loss: 1.56020 valid_loss: 0.76750 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.2850678401688735\n",
            "[   7/1000] train_loss: 0.46712 valid_loss: 0.28507 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.20214230163643757\n",
            "[   8/1000] train_loss: 0.23361 valid_loss: 0.20214 \n",
            "Saving Model...\n",
            "min_valid_loss: 0.19036231841892004\n",
            "[   9/1000] train_loss: 0.17508 valid_loss: 0.19036 \n",
            "Saving Model...\n",
            "[  10/1000] train_loss: 0.17315 valid_loss: 0.19060 \n",
            "patience: 1\n",
            "[  11/1000] train_loss: 0.17039 valid_loss: 0.20319 \n",
            "patience: 2\n",
            "[  12/1000] train_loss: 0.16628 valid_loss: 0.19073 \n",
            "patience: 3\n",
            "min_valid_loss: 0.18913318760072192\n",
            "[  13/1000] train_loss: 0.16638 valid_loss: 0.18913 \n",
            "Saving Model...\n",
            "[  14/1000] train_loss: 0.16736 valid_loss: 0.19092 \n",
            "patience: 1\n",
            "min_valid_loss: 0.18855054335047802\n",
            "[  15/1000] train_loss: 0.16632 valid_loss: 0.18855 \n",
            "Saving Model...\n",
            "[  16/1000] train_loss: 0.16614 valid_loss: 0.19936 \n",
            "patience: 1\n",
            "[  17/1000] train_loss: 0.16675 valid_loss: 0.18942 \n",
            "patience: 2\n",
            "min_valid_loss: 0.1877616161170105\n",
            "[  18/1000] train_loss: 0.16632 valid_loss: 0.18776 \n",
            "Saving Model...\n",
            "[  19/1000] train_loss: 0.16707 valid_loss: 0.18777 \n",
            "patience: 1\n",
            "[  20/1000] train_loss: 0.16606 valid_loss: 0.20067 \n",
            "patience: 2\n",
            "[  21/1000] train_loss: 0.16598 valid_loss: 0.18872 \n",
            "patience: 3\n",
            "[  22/1000] train_loss: 0.16683 valid_loss: 0.18932 \n",
            "patience: 4\n",
            "[  23/1000] train_loss: 0.17222 valid_loss: 0.18862 \n",
            "patience: 5\n",
            "[  24/1000] train_loss: 0.16640 valid_loss: 0.19200 \n",
            "patience: 6\n",
            "[  25/1000] train_loss: 0.16707 valid_loss: 0.18946 \n",
            "patience: 7\n",
            "[  26/1000] train_loss: 0.16629 valid_loss: 0.19263 \n",
            "patience: 8\n",
            "[  27/1000] train_loss: 0.16654 valid_loss: 0.18813 \n",
            "patience: 9\n",
            "[  28/1000] train_loss: 0.16618 valid_loss: 0.18947 \n",
            "patience: 10\n",
            "[  29/1000] train_loss: 0.16662 valid_loss: 0.18897 \n",
            "patience: 11\n",
            "[  30/1000] train_loss: 0.16653 valid_loss: 0.21171 \n",
            "patience: 12\n",
            "[  31/1000] train_loss: 0.16700 valid_loss: 0.19026 \n",
            "patience: 13\n",
            "[  32/1000] train_loss: 0.16637 valid_loss: 0.21226 \n",
            "patience: 14\n",
            "[  33/1000] train_loss: 0.16678 valid_loss: 0.18903 \n",
            "patience: 15\n",
            "[  34/1000] train_loss: 0.16729 valid_loss: 0.20073 \n",
            "patience: 16\n",
            "[  35/1000] train_loss: 0.16645 valid_loss: 0.18964 \n",
            "patience: 17\n",
            "[  36/1000] train_loss: 0.16670 valid_loss: 0.18841 \n",
            "patience: 18\n",
            "[  37/1000] train_loss: 0.16671 valid_loss: 0.18784 \n",
            "patience: 19\n",
            "[  38/1000] train_loss: 0.16698 valid_loss: 0.18944 \n",
            "patience: 20\n",
            "[  39/1000] train_loss: 0.16652 valid_loss: 0.19011 \n",
            "patience: 21\n",
            "[  40/1000] train_loss: 0.16694 valid_loss: 0.18902 \n",
            "patience: 22\n",
            "[  41/1000] train_loss: 0.16698 valid_loss: 0.19267 \n",
            "patience: 23\n",
            "[  42/1000] train_loss: 0.16598 valid_loss: 0.21070 \n",
            "patience: 24\n",
            "[  43/1000] train_loss: 0.16651 valid_loss: 0.19117 \n",
            "patience: 25\n",
            "[  44/1000] train_loss: 0.16619 valid_loss: 0.19836 \n",
            "patience: 26\n",
            "[  45/1000] train_loss: 0.16687 valid_loss: 0.19101 \n",
            "patience: 27\n",
            "[  46/1000] train_loss: 0.16744 valid_loss: 0.19990 \n",
            "patience: 28\n",
            "[  47/1000] train_loss: 0.16608 valid_loss: 0.20936 \n",
            "patience: 29\n",
            "[  48/1000] train_loss: 0.17215 valid_loss: 0.18934 \n",
            "patience: 30\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "humi_predict_model = train_model(humiModel, patience=30, num_epochs=1000,\n",
        "                                 train_loader=humi_trainloader,\n",
        "                                 valid_loader=humi_validloader)\n",
        "torch.save(humi_predict_model.state_dict(), \"humi_predict.pt\")"
      ],
      "id": "TDeF6FwT_SS5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgZm468V_SS5"
      },
      "source": [
        "### ONNX Coversion"
      ],
      "id": "wgZm468V_SS5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c64233ee",
      "metadata": {
        "id": "c64233ee"
      },
      "outputs": [],
      "source": [
        "def convert_to_onnx(saved_file_name):\n",
        "    saved_model = SimpleModel()\n",
        "    saved_model.load_state_dict(torch.load(saved_file_name))\n",
        "    saved_model.eval()\n",
        "    torch.onnx.export(\n",
        "        saved_model,\n",
        "        torch.randn((1, 3, 1)),\n",
        "        saved_file_name[:-3] + '.onnx',\n",
        "        opset_version=11,\n",
        "        do_constant_folding=True,\n",
        "        input_names=[\"input\"],\n",
        "        output_names=[\"output\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oZ5RhmY_SS5"
      },
      "outputs": [],
      "source": [
        "convert_to_onnx(\"temp_predict.pt\")\n",
        "convert_to_onnx(\"humi_predict.pt\")"
      ],
      "id": "0oZ5RhmY_SS5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980f3ab3",
      "metadata": {
        "id": "980f3ab3"
      },
      "outputs": [],
      "source": [
        "# End of Document."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JpTB3du5BIPo"
      },
      "id": "JpTB3du5BIPo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
